{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # NeurIPS 2025 â€“ Open Polymer Prediction\n",
    "\n",
    " ### ChemBERTa baseline (multi-task, masked wMAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Environment & installs\n",
    "\n",
    " On Kaggle you normally pre-install `transformers`, but\n",
    "\n",
    " include the `pip` line for local / VS Code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Imports & configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, math, re, gc, warnings, pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED          = 42\n",
    "MODEL_NAME    = \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "BATCH_SIZE    = 16          # fits comfortably on Kaggle T4/V100\n",
    "MAX_LEN       = 128\n",
    "EPOCHS        = 8\n",
    "LR            = 2e-5\n",
    "WARMUP_RATIO  = 0.1\n",
    "OUTPUT_DIR    = Path(\"./checkpoints\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "TARGETS = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\") # change this device if you use GPU\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3  Load & clean data\n",
    "\n",
    " *Remove only rows with completely missing SMILES; keep all NaNs in targets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.getcwd()\n",
    "\n",
    "# load the data\n",
    "train_csv = os.path.join(DATA_DIR, \"train.csv\")\n",
    "test_csv  = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_test  = pd.read_csv(test_csv)\n",
    "\n",
    "\n",
    "print(f\"Train shape  : {df_train.shape}\")\n",
    "print(f\"Test  shape  : {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load External Data Files from RDKit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdmolops\n",
    "\n",
    "def make_smi_canonical(smi):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        canon_smi = Chem.MolToSmiles(mol, canonical=True)\n",
    "        return canon_smi\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SMILES matching from rdkit\n",
    "df_train['SMILES'] = df_train['SMILES'].apply(lambda x: make_smi_canonical(x))\n",
    "df_test['SMILES'] = df_test['SMILES'].apply(lambda x: make_smi_canonical(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add External Data Files From Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tc_SMILES data\n",
    "data_tc = pd.read_csv(os.path.join(DATA_DIR,'Tc_SMILES.csv'))\n",
    "data_tc = data_tc.rename(columns={'TC_mean': 'Tc'})\n",
    "\n",
    "# Load JCIM data\n",
    "data_tg2 = pd.read_csv(os.path.join(DATA_DIR,'JCIM_sup_bigsmiles.csv'), usecols=['SMILES', 'Tg (C)'])\n",
    "data_tg2 = data_tg2.rename(columns={'Tg (C)': 'Tg'})\n",
    "\n",
    "# Load external excel sheets\n",
    "data_tg3 = pd.read_excel(os.path.join(DATA_DIR, 'data_tg3.xlsx'))\n",
    "data_tg3 = data_tg3.rename(columns={'Tg [K]': 'Tg'})\n",
    "data_tg3['Tg'] = data_tg3['Tg'] - 273.15 # Adjusting Tg values from Kelvin to Celsius\n",
    "data_dnst = pd.read_excel(os.path.join(DATA_DIR, 'data_dnst1.xlsx'))\n",
    "data_dnst = data_dnst.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n",
    "\n",
    "#Apply SMILES transform from RDKit\n",
    "print(\"Applying canonical SMILES conversion to extra datasets...\")\n",
    "data_dnst['SMILES'] = data_dnst['SMILES'].apply(lambda s: make_smi_canonical(s))\n",
    "print(\"SMILE conversion complete.\")\n",
    "\n",
    "# Adjust to fit internal dataset format\n",
    "data_dnst = data_dnst[(data_dnst['SMILES'].notnull())&(data_dnst['Density'].notnull())&(data_dnst['Density'] != 'nylon')]\n",
    "data_dnst['Density'] = data_dnst['Density'].astype('float64')\n",
    "data_dnst['Density'] -= 0.118 # Adjusting density values to match the competition's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_data(df_train, df_extra, target):\n",
    "    n_samples_before = len(df_train[df_train[target].notnull()])\n",
    "    print(f\"Applying canonical SMILES conversion for {target}...\")\n",
    "    df_extra['SMILES'] = df_extra['SMILES'].apply(lambda s: make_smi_canonical(s))\n",
    "    print(f\"SMILE conversion complete for {target}.\")\n",
    "    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n",
    "    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n",
    "\n",
    "    # Make priority target value from competition's df\n",
    "    for smile in df_train[df_train[target].notnull()]['SMILES'].tolist():\n",
    "        if smile in cross_smiles:\n",
    "            cross_smiles.remove(smile)\n",
    "\n",
    "    # Imput missing values for competition's SMILES\n",
    "    for smile in cross_smiles:\n",
    "        df_train.loc[df_train['SMILES']==smile, target] = df_extra[df_extra['SMILES']==smile][target].values[0]\n",
    "    \n",
    "    df_train = pd.concat([df_train, df_extra[df_extra['SMILES'].isin(unique_smiles_extra)]], axis=0).reset_index(drop=True)\n",
    "\n",
    "    n_samples_after = len(df_train[df_train[target].notnull()])\n",
    "    print(f'\\nFor target \"{target}\" added {n_samples_after-n_samples_before} new samples!')\n",
    "    print(f'New unique SMILES: {len(unique_smiles_extra)}')\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_extra_data(df_train, data_tc, 'Tc')\n",
    "train = add_extra_data(train, data_tg2, 'Tg')\n",
    "train = add_extra_data(train, data_tg3, 'Tg')\n",
    "train = add_extra_data(train, data_dnst, 'Density')\n",
    "\n",
    "print('\\n'*3, '--- SMILES for training ---', )\n",
    "for t in TARGETS:\n",
    "    print(f'\"{t}\": {len(train[train[t].notnull()])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "\n",
    "This code is collected from the kaggle notebook (includes a lot of domain-based preprocessing steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "useless_cols = [    \n",
    "    # Nan data\n",
    "    'BCUT2D_MWHI',\n",
    "    'BCUT2D_MWLOW',\n",
    "    'BCUT2D_CHGHI',\n",
    "    'BCUT2D_CHGLO',\n",
    "    'BCUT2D_LOGPHI',\n",
    "    'BCUT2D_LOGPLOW',\n",
    "    'BCUT2D_MRHI',\n",
    "    'BCUT2D_MRLOW',\n",
    "\n",
    "    # Constant data\n",
    "    'NumRadicalElectrons',\n",
    "    'SMR_VSA8',\n",
    "    'SlogP_VSA9',\n",
    "    'fr_barbitur',\n",
    "    'fr_benzodiazepine',\n",
    "    'fr_dihydropyridine',\n",
    "    'fr_epoxide',\n",
    "    'fr_isothiocyan',\n",
    "    'fr_lactam',\n",
    "    'fr_nitroso',\n",
    "    'fr_prisulfonamd',\n",
    "    'fr_thiocyan',\n",
    "\n",
    "    # High correlated data >0.95\n",
    "    'MaxEStateIndex',\n",
    "    'HeavyAtomMolWt',\n",
    "    'ExactMolWt',\n",
    "    'NumValenceElectrons',\n",
    "    'Chi0',\n",
    "    'Chi0n',\n",
    "    'Chi0v',\n",
    "    'Chi1',\n",
    "    'Chi1n',\n",
    "    'Chi1v',\n",
    "    'Chi2n',\n",
    "    'Kappa1',\n",
    "    'LabuteASA',\n",
    "    'HeavyAtomCount',\n",
    "    'MolMR',\n",
    "    'Chi3n',\n",
    "    'BertzCT',\n",
    "    'Chi2v',\n",
    "    'Chi4n',\n",
    "    'HallKierAlpha',\n",
    "    'Chi3v',\n",
    "    'Chi4v',\n",
    "    'MinAbsPartialCharge',\n",
    "    'MinPartialCharge',\n",
    "    'MaxAbsPartialCharge',\n",
    "    'FpDensityMorgan2',\n",
    "    'FpDensityMorgan3',\n",
    "    'Phi',\n",
    "    'Kappa3',\n",
    "    'fr_nitrile',\n",
    "    'SlogP_VSA6',\n",
    "    'NumAromaticCarbocycles',\n",
    "    'NumAromaticRings',\n",
    "    'fr_benzene',\n",
    "    'VSA_EState6',\n",
    "    'NOCount',\n",
    "    'fr_C_O',\n",
    "    'fr_C_O_noCOO',\n",
    "    'NumHDonors',\n",
    "    'fr_amide',\n",
    "    'fr_Nhpyrrole',\n",
    "    'fr_phenol',\n",
    "    'fr_phenol_noOrthoHbond',\n",
    "    'fr_COO2',\n",
    "    'fr_halogen',\n",
    "    'fr_diazo',\n",
    "    'fr_nitro_arom',\n",
    "    'fr_phos_ester'\n",
    "]\n",
    "\n",
    "def compute_all_descriptors(smiles, desc_names):\n",
    "    # Add this check to handle non-string types like float/NaN\n",
    "    if not isinstance(smiles, str):\n",
    "        return [None] * len(desc_names)\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return [None] * len(desc_names)\n",
    "    return [desc[1](mol) for desc in Descriptors.descList if desc[0] not in useless_cols]\n",
    "\n",
    "def compute_graph_features(smiles, graph_feats):\n",
    "    # Add this check to handle non-string types like float/NaN\n",
    "    if not isinstance(smiles, str):\n",
    "        graph_feats['graph_diameter'].append(0)\n",
    "        graph_feats['avg_shortest_path'].append(0)\n",
    "        graph_feats['num_cycles'].append(0)\n",
    "        return\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    # Add a check in case RDKit fails to parse a valid string\n",
    "    if mol is None:\n",
    "        graph_feats['graph_diameter'].append(0)\n",
    "        graph_feats['avg_shortest_path'].append(0)\n",
    "        graph_feats['num_cycles'].append(0)\n",
    "        return\n",
    "        \n",
    "    adj = rdmolops.GetAdjacencyMatrix(mol)\n",
    "    G = nx.from_numpy_array(adj)\n",
    "\n",
    "    graph_feats['graph_diameter'].append(nx.diameter(G) if nx.is_connected(G) else 0)\n",
    "    graph_feats['avg_shortest_path'].append(nx.average_shortest_path_length(G) if nx.is_connected(G) else 0)\n",
    "    graph_feats['num_cycles'].append(len(list(nx.cycle_basis(G))))\n",
    "\n",
    "def preprocessing(df):\n",
    "    desc_names = [desc[0] for desc in Descriptors.descList if desc[0] not in useless_cols]\n",
    "    descriptors = [compute_all_descriptors(smi, desc_names=desc_names) for smi in df['SMILES'].to_list()]\n",
    "\n",
    "    graph_feats = {'graph_diameter': [], 'avg_shortest_path': [], 'num_cycles': []}\n",
    "    for smile in df['SMILES']:\n",
    "         compute_graph_features(smile, graph_feats)\n",
    "        \n",
    "    result = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(descriptors, columns=desc_names),\n",
    "            pd.DataFrame(graph_feats)\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    result = result.replace([-np.inf, np.inf], np.nan)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, preprocessing(train)], axis=1)\n",
    "test = pd.concat([df_test, preprocessing(df_test)], axis=1)\n",
    "\n",
    "# Find constant columns for each target\n",
    "all_features = train.columns[7:].tolist()\n",
    "features = {}\n",
    "for target in TARGETS:\n",
    "    const_descs = []\n",
    "    for col in train.columns.drop(TARGETS):\n",
    "        if train[train[target].notnull()][col].nunique() == 1:\n",
    "            const_descs.append(col)\n",
    "    features[target] = [f for f in all_features if f not in const_descs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of train:\", train.shape)\n",
    "print(\"size of not null Tg in train:\", train['Tg'].notnull().sum())\n",
    "print(\"size of not null FFV in train:\", train['FFV'].notnull().sum())\n",
    "print(\"size of not null Tc in train:\", train['Tc'].notnull().sum())\n",
    "print(\"size of not null Density in train:\", train['Density'].notnull().sum())\n",
    "print(\"size of not null Rg in train:\", train['Rg'].notnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"features: \", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modelling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Tokeniser Embeddings (ChemBERTa) + Features (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "bert_model.eval() # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_bert_embeddings(smiles_list, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates ChemBERTa embeddings for a list of SMILES strings.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    # Process in batches to handle large datasets\n",
    "    for i in tqdm(range(0, len(smiles_list), BATCH_SIZE), desc=\"Generating Embeddings\"):\n",
    "        batch_smiles = smiles_list[i:i+BATCH_SIZE]\n",
    "        inputs = tokenizer(\n",
    "            batch_smiles,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get last hidden state and extract the [CLS] token embedding (index 0)\n",
    "        outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate and Store Embeddings ---\n",
    "# Assuming 'train' and 'test' DataFrames are already loaded and preprocessed\n",
    "train_smiles = train[\"SMILES\"].fillna(\"\").tolist()\n",
    "test_smiles = test[\"SMILES\"].fillna(\"\").tolist()\n",
    "\n",
    "train_embeddings = get_bert_embeddings(train_smiles, bert_model, tokenizer)\n",
    "test_embeddings = get_bert_embeddings(test_smiles, bert_model, tokenizer)\n",
    "\n",
    "# Create new DataFrames from the embeddings\n",
    "embedding_cols = [f\"bert_{i}\" for i in range(train_embeddings.shape[1])]\n",
    "train_embeddings_df = pd.DataFrame(train_embeddings, columns=embedding_cols, index=train.index)\n",
    "test_embeddings_df = pd.DataFrame(test_embeddings, columns=embedding_cols, index=test.index)\n",
    "\n",
    "print(\"ChemBERTa embeddings generated successfully.\")\n",
    "\n",
    "# Show embeddings example\n",
    "train_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge Embeddings with Original Features ---\n",
    "# Concatenate the original dataframes with the new embedding dataframes\n",
    "train_combined = pd.concat([train, train_embeddings_df], axis=1)\n",
    "test_combined = pd.concat([test, test_embeddings_df], axis=1)\n",
    "\n",
    "# A quick check before we modify the dictionary\n",
    "print(f\"Number of original features for 'Tg': {len(features['Tg'])}\")\n",
    "\n",
    "# Loop through each target in your dictionary and add the embedding features\n",
    "for target_name in features:\n",
    "    # Get the original list of engineered features for the current target\n",
    "    original_engineered_features = features[target_name]\n",
    "    \n",
    "    # Create the new, combined list\n",
    "    features[target_name] = original_engineered_features + embedding_cols\n",
    "\n",
    "print(\"--- Update Complete ---\")\n",
    "print(f\"New number of features for 'Tg': {len(features['Tg'])}\")\n",
    "print(f\"New number of features for 'FFV': {len(features['FFV'])}\")\n",
    "print(f\"New number of features for 'Tc': {len(features['Tc'])}\")\n",
    "print(f\"New number of features for 'Density': {len(features['Density'])}\")\n",
    "print(f\"New number of features for 'Rg': {len(features['Rg'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE Metric Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Absolute Error (MAE) loss function.\n",
    "    Args:\n",
    "        y_true (torch.Tensor): True target values.\n",
    "        y_pred (torch.Tensor): Predicted target values.\n",
    "    Returns:\n",
    "        torch.Tensor: Computed MAE loss.\n",
    "    \"\"\"\n",
    "    return sum(abs(true - pred) for true, pred in zip(y_true, y_pred)) / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Training Loop for Model (LightGBM External Features + BERT Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lightgbm\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# --- Configuration from your original notebook ---\n",
    "FOLDS = 5\n",
    "\n",
    "base_params = {\n",
    "    'device_type': 'cpu', # change to GPU if available\n",
    "    'n_estimators': 1_000_000,\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'mae',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 50,\n",
    "    'min_data_in_leaf': 2,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_bin': 500,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 2,\n",
    "    'lambda_l2': 2,\n",
    "}\n",
    "\n",
    "# --- Main Training Loop (using your code structure) ---\n",
    "for target in TARGETS:\n",
    "    print(f'TARGET {target}')\n",
    "    # Use the new dataframe with combined features\n",
    "    train_part = train_combined[train_combined[target].notnull()].reset_index(drop=True)\n",
    "    \n",
    "    # Initialize prediction columns\n",
    "    test_combined[target] = 0\n",
    "    oof_lgb = np.zeros(len(train_part))\n",
    "    scores = []\n",
    "    \n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_part, train_part[target])):\n",
    "        print(f\"--- Fold {i+1} ---\")\n",
    "        \n",
    "        # Use the comprehensive feature list for training and validation sets\n",
    "        x_trn = train_part.loc[trn_idx, features[target]]\n",
    "        y_trn = train_part.loc[trn_idx, target]\n",
    "        x_val = train_part.loc[val_idx, features[target]]\n",
    "        y_val = train_part.loc[val_idx, target]\n",
    "\n",
    "        model_lgb = lgb.LGBMRegressor(**base_params)\n",
    "        model_lgb.fit(\n",
    "            x_trn, y_trn,\n",
    "            eval_set=[(x_val, y_val)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(\n",
    "                    stopping_rounds=300,\n",
    "                    verbose=False,\n",
    "                ),\n",
    "                lgb.log_evaluation(2500)\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        # Save the trained model\n",
    "        with open(f'lgb_{target}_fold_{i}.pkl', 'wb') as f:\n",
    "            pickle.dump(model_lgb, f)\n",
    "\n",
    "        val_preds = model_lgb.predict(x_val, num_iteration=model_lgb.best_iteration_)\n",
    "        score = mae(y_val, val_preds)\n",
    "        scores.append(score)\n",
    "        print(f'MAE: {np.round(score, 5)}')\n",
    "        \n",
    "        oof_lgb[val_idx] = val_preds\n",
    "        \n",
    "        # Predict on the test set (which also includes embeddings)\n",
    "        test_combined[target] += model_lgb.predict(\n",
    "            test_combined[features[target]], \n",
    "            num_iteration=model_lgb.best_iteration_\n",
    "        ) / FOLDS\n",
    "\n",
    "    # You might want to save the OOF predictions for later analysis\n",
    "    train_combined.loc[train_combined[target].notnull(), f'{target}_pred'] = oof_lgb\n",
    "\n",
    "    print(f'Mean MAE: {np.round(np.mean(scores), 5)}')\n",
    "    print(f'Std MAE: {np.round(np.std(scores), 5)}')\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"--- Visualizing Out-of-Fold Predictions vs. Actuals ---\")\n",
    "\n",
    "for t in TARGETS:\n",
    "    # Use your main dataframe containing the predictions\n",
    "    plot_df = train_combined[train_combined[t].notnull()]\n",
    "    \n",
    "    preds = plot_df[f'{t}_pred']\n",
    "    vals = plot_df[t]\n",
    "    \n",
    "    # Determine the plot limits to make a square plot\n",
    "    line_min = min(preds.min(), vals.min())\n",
    "    line_max = max(preds.max(), vals.max())\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.scatterplot(x=preds, y=vals, alpha=0.5)\n",
    "    \n",
    "    # Add the y=x line for reference (a perfect model)\n",
    "    plt.plot([line_min, line_max], [line_min, line_max], color='red', linewidth=2, linestyle='dashed')\n",
    "    \n",
    "    plt.title(f'Predictions vs. Actuals for {t}')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Using Metrics From the Competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Minimum and Mamimum value of each Target Property (Handcoded)\n",
    "MINMAX_DICT =  {\n",
    "    'Tg': [-148.0297376, 472.25],\n",
    "    'FFV': [0.2269924, 0.77709707],\n",
    "    'Tc': [0.0465, 0.524],\n",
    "    'Density': [0.748691234, 1.840998909],\n",
    "    'Rg': [9.7283551, 34.672905605],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_error(labels, preds, property):\n",
    "    # Function to rescale labels\n",
    "    error = np.abs(labels - preds)\n",
    "    min_val, max_val = MINMAX_DICT[property]\n",
    "    label_range = max_val - min_val\n",
    "    return np.mean(error / label_range)\n",
    "\n",
    "def get_property_weights(labels):\n",
    "    property_weight = []\n",
    "    for property in MINMAX_DICT.keys():\n",
    "        # Changed this line to use .notna().sum()\n",
    "        valid_num = labels[property].notna().sum()\n",
    "        property_weight.append(valid_num)\n",
    "        \n",
    "    property_weight = np.array(property_weight)\n",
    "    property_weight = np.sqrt(1 / property_weight)\n",
    "    return (property_weight / np.sum(property_weight)) * len(property_weight)\n",
    "\n",
    "# MODIFIED to correctly handle NaN values instead of -9999\n",
    "def wmae_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    # Weighted MAE: Metric Used by the Competition\n",
    "    chemical_properties = list(MINMAX_DICT.keys())\n",
    "    property_maes = []\n",
    "    property_weights = get_property_weights(solution[chemical_properties])\n",
    "    \n",
    "    for property in chemical_properties:\n",
    "        # Changed this line to use .notna()\n",
    "        is_labeled = solution[property].notna()\n",
    "        property_maes.append(scaling_error(solution.loc[is_labeled, property], submission.loc[is_labeled, property], property))\n",
    "\n",
    "    if len(property_maes) == 0:\n",
    "        raise RuntimeError('No labels')\n",
    "        \n",
    "    return float(np.average(property_maes, weights=property_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the solution dataframe (true values) from your main dataframe\n",
    "tr_solution = train_combined[['id'] + TARGETS]\n",
    "\n",
    "# Prepare the submission dataframe (OOF predictions)\n",
    "pred_cols = [t + '_pred' for t in TARGETS]\n",
    "tr_submission = train_combined[['id'] + pred_cols]\n",
    "\n",
    "# Rename prediction columns to match solution columns (e.g., 'Tg_pred' -> 'Tg')\n",
    "tr_submission.columns = ['id'] + TARGETS\n",
    "\n",
    "# Calculate and print the final wMAE score\n",
    "final_score = wmae_score(tr_solution, tr_submission, row_id_column_name='id')\n",
    "print(f\"Final Out-of-Fold wMAE Score: {round(final_score, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance & Correlation Anlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "FOLDS = 5\n",
    "N_TOP_FEATURES = 50  # Define how many of the best independent features you want to keep\n",
    "CORRELATION_THRESHOLD = 0.75 # Define the threshold for removing redundant features\n",
    "\n",
    "# STEP 1: LOAD MODELS AND CALCULATE AVERAGE FEATURE IMPORTANCES\n",
    "all_feature_importances = {}\n",
    "master_feature_list = set()\n",
    "\n",
    "print(\"--- Step 1: Loading Models and Calculating Feature Importances ---\")\n",
    "for target in TARGETS:\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i in range(FOLDS):\n",
    "        with open(f'lgb_{target}_fold_{i}.pkl', 'rb') as f:\n",
    "            model_lgb = pickle.load(f)\n",
    "        \n",
    "        fold_df = pd.DataFrame()\n",
    "        fold_df[\"feature\"] = model_lgb.feature_name_\n",
    "        fold_df[\"importance\"] = model_lgb.feature_importances_\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_df], ignore_index=True)\n",
    "    \n",
    "    mean_importance_df = feature_importance_df.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).reset_index()\n",
    "    all_feature_importances[target] = mean_importance_df\n",
    "    master_feature_list.update(mean_importance_df['feature'])\n",
    "\n",
    "print(f\"Importance scores calculated for {len(master_feature_list)} unique features.\")\n",
    "\n",
    "# STEP 2: IDENTIFY AND REMOVE REDUNDANT FEATURES (FROM THE ENTIRE SET)\n",
    "print(\"\\n--- Step 2: Identifying Redundant Features ---\")\n",
    "corr_matrix = train_combined[list(master_feature_list)].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "highly_correlated_features = [column for column in upper_tri.columns if any(upper_tri[column] > CORRELATION_THRESHOLD)]\n",
    "features_to_drop = set()\n",
    "\n",
    "for feature in highly_correlated_features:\n",
    "    correlated_with_feature = upper_tri[feature][upper_tri[feature] > CORRELATION_THRESHOLD].index.tolist()\n",
    "    group = correlated_with_feature + [feature]\n",
    "    \n",
    "    group_importances = {}\n",
    "    for f in group:\n",
    "        total_importance = sum(\n",
    "            all_feature_importances[target].loc[all_feature_importances[target]['feature'] == f, 'importance'].iloc[0]\n",
    "            for target in TARGETS if f in all_feature_importances[target]['feature'].values\n",
    "        )\n",
    "        group_importances[f] = total_importance / len(TARGETS)\n",
    "        \n",
    "    feature_to_keep = max(group_importances, key=group_importances.get)\n",
    "    for f in group:\n",
    "        if f != feature_to_keep:\n",
    "            features_to_drop.add(f)\n",
    "\n",
    "print(f\"Identified {len(features_to_drop)} redundant features to drop from the master list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Top N Features Per Target\n",
    "SELECT TOP N INDEPENDENT FEATURES FOR EACH TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature lists for retraining.\n",
    "print(\"\\n--- Step 3: Selecting Top N Independent Features per Target ---\")\n",
    "final_feature_sets = {}\n",
    "for target in TARGETS:\n",
    "    # Start with the full ranked list of importances for this target\n",
    "    ranked_df = all_feature_importances[target]\n",
    "    \n",
    "    # Filter out the features that were marked as redundant\n",
    "    independent_features_df = ranked_df[~ranked_df['feature'].isin(features_to_drop)]\n",
    "    \n",
    "    # select the Top N from the remaining independent features\n",
    "    final_feature_sets[target] = independent_features_df.head(N_TOP_FEATURES)['feature'].tolist()\n",
    "    \n",
    "    print(f\"For target '{target}', selected {len(final_feature_sets[target])} final independent features.\")\n",
    "\n",
    "    # Visualize the final selected features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=independent_features_df.head(50))\n",
    "    plt.title(f\"Top 50 Independent Feature Importances for {target}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Model with N Independent Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- RETRAINING WITH FINAL INDEPENDENT FEATURES ---\")\n",
    "\n",
    "# --- Main Retraining Loop ---\n",
    "for target in TARGETS:\n",
    "    print(f'\\nTARGET {target}')\n",
    "    train_part = train_combined[train_combined[target].notnull()].reset_index(drop=True)\n",
    "    \n",
    "    # Initialize prediction columns in the test set\n",
    "    test_combined[target] = 0\n",
    "    oof_lgb = np.zeros(len(train_part))\n",
    "    scores = []\n",
    "    \n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_part, train_part[target])):\n",
    "        print(f\"--- Fold {i+1} ---\")\n",
    "        \n",
    "        # --- CHANGED: Use the 'final_feature_sets' dictionary ---\n",
    "        x_trn = train_part.loc[trn_idx, final_feature_sets[target]]\n",
    "        y_trn = train_part.loc[trn_idx, target]\n",
    "        x_val = train_part.loc[val_idx, final_feature_sets[target]]\n",
    "        y_val = train_part.loc[val_idx, target]\n",
    "\n",
    "        model_lgb = lgb.LGBMRegressor(**base_params)\n",
    "        model_lgb.fit(\n",
    "            x_trn, y_trn,\n",
    "            eval_set=[(x_val, y_val)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(\n",
    "                    stopping_rounds=300,\n",
    "                    verbose=False,\n",
    "                ),\n",
    "                lgb.log_evaluation(2500)\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        # Save the final, retrained models with a new name\n",
    "        with open(f'lgb_final_{target}_fold_{i}.pkl', 'wb') as f:\n",
    "            pickle.dump(model_lgb, f)\n",
    "\n",
    "        val_preds = model_lgb.predict(x_val, num_iteration=model_lgb.best_iteration_)\n",
    "        score = mae(y_val, val_preds)\n",
    "        scores.append(score)\n",
    "        print(f'MAE: {np.round(score, 5)}')\n",
    "        \n",
    "        oof_lgb[val_idx] = val_preds\n",
    "        \n",
    "        # --- CHANGED: Also use 'final_feature_sets' for test set prediction ---\n",
    "        test_combined[target] += model_lgb.predict(\n",
    "            test_combined[final_feature_sets[target]], \n",
    "            num_iteration=model_lgb.best_iteration_\n",
    "        ) / FOLDS\n",
    "\n",
    "    # Save the new OOF predictions with a new column name for comparison\n",
    "    train_combined.loc[train_combined[target].notnull(), f'{target}_final_pred'] = oof_lgb\n",
    "\n",
    "    print(f'\\nMean MAE with final features: {np.round(np.mean(scores), 5)}')\n",
    "    print(f'Std MAE: {np.round(np.std(scores), 5)}')\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TRAINING WITH ENGINEERED INDEPENDENT FEATURES ONLY + LIGHTGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors, rdmolops\n",
    "\n",
    "def compute_graph_features(smiles):\n",
    "    # VERSION 2 of the graph features function to handle nan inputs\n",
    "    if not isinstance(smiles, str):\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    adj = rdmolops.GetAdjacencyMatrix(mol)\n",
    "    G = nx.from_numpy_array(adj)\n",
    "    \n",
    "    diameter = nx.diameter(G) if nx.is_connected(G) else 0\n",
    "    avg_path = nx.average_shortest_path_length(G) if nx.is_connected(G) else 0\n",
    "    cycles = len(list(nx.cycle_basis(G)))\n",
    "    return [diameter, avg_path, cycles]\n",
    "\n",
    "def preprocessing(df):\n",
    "    # RDKit descriptors\n",
    "    desc_names = [desc[0] for desc in Descriptors.descList if desc[0] not in useless_cols]\n",
    "    descriptors = [compute_all_descriptors(smi) for smi in df['SMILES']]\n",
    "    desc_df = pd.DataFrame(descriptors, columns=desc_names, index=df.index)\n",
    "    \n",
    "    # Graph features\n",
    "    graph_feats = [compute_graph_features(smi) for smi in df['SMILES']]\n",
    "    graph_df = pd.DataFrame(graph_feats, columns=['graph_diameter', 'avg_shortest_path', 'num_cycles'], index=df.index)\n",
    "    \n",
    "    result = pd.concat([desc_df, graph_df], axis=1)\n",
    "    result = result.replace([-np.inf, np.inf], np.nan)\n",
    "    return result\n",
    "\n",
    "# Assume 'train_raw' and 'test_raw' are your initial dataframes before feature engineering\n",
    "# train = pd.concat([train_raw, preprocessing(train_raw)], axis=1)\n",
    "# test = pd.concat([test_raw, preprocessing(test_raw)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE FEATURE LISTS (ENGINEERED FEATURES ONLY)\n",
    "\n",
    "# This logic automatically selects all newly created engineered features\n",
    "all_features = [col for col in train.columns if col not in ['id', 'SMILES'] + TARGETS]\n",
    "features = {}\n",
    "\n",
    "for target in TARGETS:\n",
    "    const_descs = []\n",
    "    # Find columns that are constant for the non-null subset of the current target\n",
    "    train_subset = train[train[target].notnull()]\n",
    "    for col in all_features:\n",
    "        if train_subset[col].nunique() == 1:\n",
    "            const_descs.append(col)\n",
    "    # The final feature list for the target excludes these constant columns\n",
    "    features[target] = [f for f in all_features if f not in const_descs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: MODEL TRAINING (USING THE ENGINEERED-ONLY FEATURE LISTS)\n",
    "\n",
    "for target in TARGETS:\n",
    "    print(f'\\n\\nTARGET {target}')\n",
    "    train_part = train[train[target].notnull()].reset_index(drop=True)\n",
    "    test[f'{target}_engineered_pred'] = 0\n",
    "    oof_preds = np.zeros(len(train_part))\n",
    "    scores = []\n",
    "    \n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_part, train_part[target])):\n",
    "        print(f\"\\n--- Fold {i+1} ---\")\n",
    "        \n",
    "        # This line uses the 'features' dictionary created in Part 2,\n",
    "        # which only contains the engineered features.\n",
    "        x_trn = train_part.loc[trn_idx, features[target]]\n",
    "        y_trn = train_part.loc[trn_idx, target]\n",
    "        x_val = train_part.loc[val_idx, features[target]]\n",
    "        y_val = train_part.loc[val_idx, target]\n",
    "\n",
    "        model_lgb = lgb.LGBMRegressor(**base_params)\n",
    "        model_lgb.fit(\n",
    "            x_trn, y_trn, eval_set=[(x_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(300, verbose=False), lgb.log_evaluation(2500)]\n",
    "        )\n",
    "\n",
    "        # --- ADDED: Save the trained model for this fold ---\n",
    "        # This allows you to load it later without retraining.\n",
    "        with open(f'lgb_engineered_only_{target}_fold_{i}.pkl', 'wb') as f:\n",
    "            pickle.dump(model_lgb, f)\n",
    "\n",
    "        val_preds = model_lgb.predict(x_val, num_iteration=model_lgb.best_iteration_)\n",
    "        score = mae(y_val, val_preds)\n",
    "        scores.append(score)\n",
    "        print(f'MAE: {np.round(score, 5)}')\n",
    "        \n",
    "        oof_preds[val_idx] = val_preds\n",
    "        test[f'{target}_engineered_pred'] += model_lgb.predict(\n",
    "            test[features[target]], \n",
    "            num_iteration=model_lgb.best_iteration_\n",
    "        ) / FOLDS\n",
    "\n",
    "    # Save the out-of-fold predictions to your main train dataframe \n",
    "    # This is crucial for plotting and analyzing your model's performance.\n",
    "    train.loc[train[target].notnull(), f'{target}_engineered_pred'] = oof_preds\n",
    "\n",
    "    print(f'\\nMean MAE: {np.round(np.mean(scores), 5)}')\n",
    "    print(f'Std MAE: {np.round(np.std(scores), 5)}')\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ADDED: Save the final test predictions to a CSV file ---\n",
    "print(\"\\nSaving final test predictions to submission_engineered_only.csv\")\n",
    "submission_cols = ['id'] + [f'{t}_engineered_pred' for t in TARGETS]\n",
    "submission_df = test[submission_cols]\n",
    "# Rename columns to match the submission format (e.g., 'Tg_engineered_pred' -> 'Tg')\n",
    "submission_df.columns = ['id'] + TARGETS\n",
    "submission_df.to_csv('submission_engineered_only.csv', index=False)\n",
    "\n",
    "print(\"Process complete. All models and results have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q optuna \n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# This dictionary will store the best parameters found for each target\n",
    "best_params_per_target = {}\n",
    "\n",
    "# Loop through each target to find its best parameters\n",
    "for target in TARGETS:\n",
    "    print(f\"\\n--- Tuning Hyperparameters for Target: {target} ---\")\n",
    "    \n",
    "    # Prepare the data for the current target\n",
    "    train_part = train[train[target].notnull()].reset_index(drop=True)\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"\n",
    "        This function defines one trial of the optimization.\n",
    "        Optuna will call this function many times with different parameter values.\n",
    "        \"\"\"\n",
    "        # Define the search space for the hyperparameters\n",
    "        params = {\n",
    "            'device_type': 'cpu',\n",
    "            'objective': 'regression_l1',\n",
    "            'metric': 'mae',\n",
    "            'verbosity': -1,\n",
    "            'n_estimators': 10000, # Use a large number, early stopping will find what's needed\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 80),\n",
    "            'max_bin': trial.suggest_int('max_bin', 200, 600),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-2, 10.0, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-2, 10.0, log=True),\n",
    "        }\n",
    "        \n",
    "        scores = []\n",
    "        kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        for i, (trn_idx, val_idx) in enumerate(kf.split(train_part)):\n",
    "            x_trn = train_part.loc[trn_idx, features[target]]\n",
    "            y_trn = train_part.loc[trn_idx, target]\n",
    "            x_val = train_part.loc[val_idx, features[target]]\n",
    "            y_val = train_part.loc[val_idx, target]\n",
    "\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            model.fit(\n",
    "                x_trn, y_trn,\n",
    "                eval_set=[(x_val, y_val)],\n",
    "                callbacks=[lgb.early_stopping(300, verbose=False)]\n",
    "            )\n",
    "            preds = model.predict(x_val)\n",
    "            scores.append(mae(y_val, preds))\n",
    "\n",
    "        # Return the average MAE across the folds for this set of parameters\n",
    "        return np.mean(scores)\n",
    "\n",
    "    # Create and run the study for the current target\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=30)  # n_trials can be increased for a more thorough search\n",
    "\n",
    "    print(f\"Best MAE for {target}: {study.best_value}\")\n",
    "    print(f\"Best parameters for {target}: {study.best_params}\")\n",
    "    best_params_per_target[target] = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdmolops\n",
    "import networkx as nx\n",
    "\n",
    "# --- Configuration ---\n",
    "TARGETS = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "FOLDS = 5\n",
    "SEED = 42\n",
    "\n",
    "# (Assuming 'train' and 'test' dataframes with engineered features are already loaded)\n",
    "# (Assuming the 'features' dictionary is also already created)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: DEFINE BOTH SETS OF PARAMETERS\n",
    "# ==============================================================================\n",
    "\n",
    "# Your original default parameters\n",
    "base_params = {\n",
    "    'device_type': 'cpu', 'n_estimators': 1_000_000, 'objective': 'regression_l1',\n",
    "    'metric': 'mae', 'verbosity': -1, 'num_leaves': 50, 'min_data_in_leaf': 2,\n",
    "    'learning_rate': 0.01, 'max_bin': 500, 'feature_fraction': 0.7, 'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1, 'lambda_l1': 2, 'lambda_l2': 2,\n",
    "}\n",
    "\n",
    "# --- ADDED: Your tuned parameters for Tg and Tc ---\n",
    "# Using the best parameters from your Optuna run\n",
    "best_params_per_target = {\n",
    "    'Tg': {\n",
    "        'learning_rate': 0.026637639805993162, 'num_leaves': 69, 'max_bin': 298,\n",
    "        'feature_fraction': 0.7859035809967311, 'bagging_fraction': 0.9159812258732549,\n",
    "        'bagging_freq': 3, 'lambda_l1': 3.8741335005648767, 'lambda_l2': 4.243674331615196\n",
    "    },\n",
    "    'Tc': {\n",
    "        'learning_rate': 0.003911894256119605, 'num_leaves': 78, 'max_bin': 461,\n",
    "        'feature_fraction': 0.6023658825194265, 'bagging_fraction': 0.6403722440012912,\n",
    "        'bagging_freq': 3, 'lambda_l1': 0.5491320841654049, 'lambda_l2': 1.3420247856754794\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: MODEL TRAINING WITH CONDITIONAL PARAMETERS\n",
    "# ==============================================================================\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n",
    "\n",
    "for target in TARGETS:\n",
    "    print(f'\\n\\nTARGET {target}')\n",
    "\n",
    "    # --- ADDED: Conditional logic to select parameters ---\n",
    "    if target in best_params_per_target:\n",
    "        print(f\"Using TUNED parameters for {target}.\")\n",
    "        params = best_params_per_target[target]\n",
    "        # Add back the fixed parameters\n",
    "        params['device_type'] = 'cpu'\n",
    "        params['objective'] = 'regression_l1'\n",
    "        params['metric'] = 'mae'\n",
    "        params['verbosity'] = -1\n",
    "        params['n_estimators'] = 1_000_000\n",
    "    else:\n",
    "        print(f\"Using BASE parameters for {target}.\")\n",
    "        params = base_params\n",
    "        \n",
    "    train_part = train[train[target].notnull()].reset_index(drop=True)\n",
    "    test[f'{target}_engineered_pred'] = 0\n",
    "    oof_preds = np.zeros(len(train_part))\n",
    "    scores = []\n",
    "    \n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_part, train_part[target])):\n",
    "        print(f\"\\n--- Fold {i+1} ---\")\n",
    "        \n",
    "        x_trn = train_part.loc[trn_idx, features[target]]\n",
    "        y_trn = train_part.loc[trn_idx, target]\n",
    "        x_val = train_part.loc[val_idx, features[target]]\n",
    "        y_val = train_part.loc[val_idx, target]\n",
    "\n",
    "        # Use the 'params' dictionary selected above\n",
    "        model_lgb = lgb.LGBMRegressor(**params)\n",
    "        model_lgb.fit(\n",
    "            x_trn, y_trn, eval_set=[(x_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(300, verbose=False), lgb.log_evaluation(2500)]\n",
    "        )\n",
    "\n",
    "        with open(f'lgb_optuna_engineered_only_{target}_fold_{i}.pkl', 'wb') as f:\n",
    "            pickle.dump(model_lgb, f)\n",
    "\n",
    "        val_preds = model_lgb.predict(x_val, num_iteration=model_lgb.best_iteration_)\n",
    "        score = mae(y_val, val_preds)\n",
    "        scores.append(score)\n",
    "        print(f'MAE: {np.round(score, 5)}')\n",
    "        \n",
    "        oof_preds[val_idx] = val_preds\n",
    "        test[f'{target}_engineered_pred'] += model_lgb.predict(\n",
    "            test[features[target]], \n",
    "            num_iteration=model_lgb.best_iteration_\n",
    "        ) / FOLDS\n",
    "\n",
    "    train.loc[train[target].notnull(), f'{target}_engineered_pred'] = oof_preds\n",
    "\n",
    "    print(f'\\nMean MAE: {np.round(np.mean(scores), 5)}')\n",
    "    print(f'Std MAE: {np.round(np.std(scores), 5)}')\n",
    "    print('-'*30)\n",
    "\n",
    "print(\"\\nSaving final test predictions to submission_engineered_only.csv\")\n",
    "submission_cols = ['id'] + [f'{t}_engineered_pred' for t in TARGETS]\n",
    "submission_df = test[submission_cols]\n",
    "submission_df.columns = ['id'] + TARGETS\n",
    "submission_df.to_csv('submission_engineered_only.csv', index=False)\n",
    "\n",
    "print(\"Process complete. All models and results have been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Model Predictions (Features Only + Optuna Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"--- Visualizing Out-of-Fold Predictions vs. Actuals ---\")\n",
    "\n",
    "for t in TARGETS:\n",
    "    # Use the 'train' dataframe and the new prediction column names\n",
    "    plot_df = train[train[t].notnull()]\n",
    "    \n",
    "    # --- CHANGED: Use the correct prediction column name ---\n",
    "    preds = plot_df[f'{t}_engineered_pred']\n",
    "    vals = plot_df[t]\n",
    "    \n",
    "    # Determine the plot limits to make a square plot\n",
    "    line_min = min(preds.min(), vals.min())\n",
    "    line_max = max(preds.max(), vals.max())\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.scatterplot(x=preds, y=vals, alpha=0.5)\n",
    "    \n",
    "    # Add the y=x line for reference (a perfect model)\n",
    "    plt.plot(\n",
    "        [line_min, line_max], \n",
    "        [line_min, line_max], \n",
    "        color='red', \n",
    "        linewidth=2, \n",
    "        linestyle='dashed'\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Predictions vs. Actuals for {t}')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Score (Feature Only + Optuna Weights Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_error(labels, preds, property):\n",
    "    error = np.abs(labels - preds)\n",
    "    min_val, max_val = MINMAX_DICT[property]\n",
    "    label_range = max_val - min_val\n",
    "    return np.mean(error / label_range)\n",
    "    \n",
    "def get_property_weights(labels):\n",
    "    property_weight = []\n",
    "    for property in MINMAX_DICT.keys():\n",
    "        valid_num = labels[property].notna().sum()\n",
    "        property_weight.append(valid_num)\n",
    "        \n",
    "    property_weight = np.array(property_weight)\n",
    "    property_weight = np.sqrt(1 / property_weight)\n",
    "    return (property_weight / np.sum(property_weight)) * len(property_weight)\n",
    "\n",
    "def wmae_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    chemical_properties = list(MINMAX_DICT.keys())\n",
    "    property_maes = []\n",
    "    property_weights = get_property_weights(solution[chemical_properties])\n",
    "    \n",
    "    for property in chemical_properties:\n",
    "        is_labeled = solution[property].notna()\n",
    "        property_maes.append(scaling_error(solution.loc[is_labeled, property], submission.loc[is_labeled, property], property))\n",
    "\n",
    "    if len(property_maes) == 0:\n",
    "        raise RuntimeError('No labels')\n",
    "        \n",
    "    return float(np.average(property_maes, weights=property_weights))\n",
    "\n",
    "# --- ADAPTED FINAL CALCULATION ---\n",
    "\n",
    "# Prepare the solution dataframe (true values) from your 'train' dataframe\n",
    "tr_solution = train[['id'] + TARGETS]\n",
    "\n",
    "# Use the correct prediction column names ---\n",
    "pred_cols = [t + '_engineered_pred' for t in TARGETS]\n",
    "tr_submission = train[['id'] + pred_cols]\n",
    "\n",
    "# Rename prediction columns to match solution columns (e.g., 'Tg_engineered_pred' -> 'Tg')\n",
    "tr_submission.columns = ['id'] + TARGETS\n",
    "\n",
    "# Calculate and print the final wMAE score\n",
    "final_score = wmae_score(tr_solution, tr_submission, row_id_column_name='id')\n",
    "print(f\"Final Out-of-Fold wMAE Score: {round(final_score, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage (Quite Unfortunate Events)\n",
    "The data from the test data is found somewhere in the added external data, this data can be iteratively searched for\n",
    " and used as the prediction to further increase the MAE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the solution dataframe (true values)\n",
    "tr_solution = train[['id'] + TARGETS]\n",
    "\n",
    "# Prepare the submission dataframe from your original OOF predictions\n",
    "pred_cols = [t + '_engineered_pred' for t in TARGETS]\n",
    "tr_submission_before_leakage = train[['id'] + pred_cols]\n",
    "tr_submission_before_leakage.columns = ['id'] + TARGETS\n",
    "\n",
    "# Calculate and print the baseline wMAE score\n",
    "base_score = wmae_score(tr_solution, tr_submission_before_leakage, row_id_column_name='id')\n",
    "print(f\"wMAE Score BEFORE Leakage Check: {round(base_score, 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of your predictions to modify\n",
    "tr_submission_after_leakage = tr_submission_before_leakage.copy()\n",
    "\n",
    "print(\"\\nApplying leakage check...\")\n",
    "leaks_found = 0\n",
    "\n",
    "# Loop through each target and apply the leakage logic\n",
    "for t in TARGETS:\n",
    "    # Get the subset of the training data where the target is known\n",
    "    train_subset = train[train[t].notnull()]\n",
    "    \n",
    "    for index, row in train_subset.iterrows():\n",
    "        smile_to_check = row['SMILES']\n",
    "        \n",
    "        # Check if this SMILES is in our prediction set (it will be, as we are using the train set)\n",
    "        mask = (tr_submission_after_leakage['id'] == row['id'])\n",
    "        if mask.any():\n",
    "            # Overwrite the model's prediction with the known true value\n",
    "            true_value = row[t]\n",
    "            tr_submission_after_leakage.loc[mask, t] = true_value\n",
    "            leaks_found += 1\n",
    "\n",
    "print(f\"Leakage check applied. Overwrote {leaks_found} values with known answers.\")\n",
    "\n",
    "# Recalculate the wMAE score with the corrected predictions\n",
    "final_score_after_leakage = wmae_score(tr_solution, tr_submission_after_leakage, row_id_column_name='id')\n",
    "print(f\"wMAE Score AFTER Leakage Check:  {round(final_score_after_leakage, 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"--- Starting Final Inference Process ---\")\n",
    "\n",
    "# --- Step 1: Load Raw Data ---\n",
    "# We need the original train data for the leakage check\n",
    "train_raw = pd.read_csv('train.csv') \n",
    "test_raw = pd.read_csv('test.csv')\n",
    "\n",
    "# --- Step 2: Apply All Preprocessing ---\n",
    "# This assumes your 'preprocessing' function is defined\n",
    "# and you have your final feature sets determined.\n",
    "print(\"Applying preprocessing to test data...\")\n",
    "# NOTE: Ensure your 'train' and 'test' dataframes are the fully preprocessed versions\n",
    "# For this example, we assume they are already created from previous steps.\n",
    "\n",
    "# This is the dictionary of features to use from your final model\n",
    "# (e.g., final_independent_features or whichever performed best)\n",
    "final_feature_sets = features # Or important_features, etc.\n",
    "\n",
    "# --- Step 3: Load Models and Predict ---\n",
    "test_predictions = test[['id']].copy()\n",
    "\n",
    "for target in TARGETS:\n",
    "    print(f\"Predicting for target: {target}\")\n",
    "    target_preds = np.zeros(len(test))\n",
    "    \n",
    "    for i in range(FOLDS):\n",
    "        # Load the correct saved model for each fold\n",
    "        # UPDATE the filename to match your best model\n",
    "        with open(f'lgb_engineered_only_{target}_fold_{i}.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        # Predict on the preprocessed test data for this fold\n",
    "        target_preds += model.predict(\n",
    "            test[final_feature_sets[target]], \n",
    "            num_iteration=model.best_iteration_\n",
    "        ) / FOLDS\n",
    "        \n",
    "    test_predictions[target] = target_preds\n",
    "\n",
    "# --- Step 4: Apply Data Leakage Check ---\n",
    "print(\"Applying data leakage check to final predictions...\")\n",
    "for t in TARGETS:\n",
    "    # Use the raw training data to find known answers\n",
    "    train_subset = train[train[t].notnull()]\n",
    "    for index, row in train_subset.iterrows():\n",
    "        smile_to_check = row['SMILES']\n",
    "        if smile_to_check in test_raw['SMILES'].values:\n",
    "            print(smile_to_check, \"Is in the row\")\n",
    "            # Find the corresponding row in the test set\n",
    "            test_mask = (test_raw['SMILES'] == smile_to_check)\n",
    "            # Overwrite our model's prediction with the known true value\n",
    "            test_predictions.loc[test_mask, t] = row[t]\n",
    "\n",
    "# --- Step 5: Format and Save Submission File ---\n",
    "submission_df = test_predictions[['id'] + TARGETS]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission.csv created successfully!\")\n",
    "print(\"Final predictions head:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['SMILES'].str.startswith('*Oc1ccc(C=NN=Cc2ccc(Oc3ccc(C')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andrej",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
