{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # NeurIPS 2025 – Open Polymer Prediction\n",
    "\n",
    " ### ChemBERTa baseline (multi-task, masked wMAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 0  Environment & installs\n",
    "\n",
    " On Kaggle you normally pre-install `transformers`, but\n",
    "\n",
    " include the `pip` line for local / VS Code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1  Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_ML/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, json, random, math, re, gc, warnings, pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AdamW,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED          = 42\n",
    "MODEL_NAME    = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "BATCH_SIZE    = 16          # fits comfortably on Kaggle T4/V100\n",
    "MAX_LEN       = 128\n",
    "EPOCHS        = 4\n",
    "LR            = 2e-5\n",
    "WARMUP_RATIO  = 0.1\n",
    "OUTPUT_DIR    = Path(\"./checkpoints\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2  Utility – weighted-MAE helper\n",
    "\n",
    " Compute the task weights \\\\(w_i\\\\) **once** from the available\n",
    "\n",
    " (public) training labels.\n",
    "\n",
    " Strictly the organisers use the **test-set** ranges, but using\n",
    "\n",
    " train-set statistics is the standard offline proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "\n",
    "def compute_task_weights(df: pd.DataFrame, eps: float = 1e-8) -> torch.Tensor:\n",
    "    ranges = df[TARGETS].max() - df[TARGETS].min()\n",
    "    n_i    = df[TARGETS].notna().sum()\n",
    "    K      = len(TARGETS)\n",
    "    root_inv_n = np.sqrt(1.0 / (n_i + eps))\n",
    "    weights = (1.0 / (ranges + eps)) * (K * root_inv_n / root_inv_n.sum())\n",
    "    return torch.tensor(weights.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3  Load & clean data\n",
    "\n",
    " *Remove only rows with completely missing SMILES; keep all NaNs in targets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape  : (7973, 7)\n",
      "Test  shape  : (3, 2)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"/Users/maxhart/Documents/AI_AND_ML/NeuralIPS-Polymer/neurips-open-polymer-prediction-2025\")  # adjust locally\n",
    "train_csv = DATA_DIR / \"train.csv\"\n",
    "test_csv  = DATA_DIR / \"test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_test  = pd.read_csv(test_csv)\n",
    "\n",
    "# basic SMILES clean-up – ChemBERTa cannot handle '*'\n",
    "def clean_smiles(s: str) -> str:\n",
    "    s = re.sub(r\"N\\*\", \"N\", s)\n",
    "    s = re.sub(r\"O\\*\", \"O\", s)\n",
    "    s = s.replace(\"*\", \"\")\n",
    "    return s\n",
    "\n",
    "df_train = df_train[df_train[\"SMILES\"].notna()].copy()\n",
    "df_train[\"SMILES\"] = df_train[\"SMILES\"].str.strip().apply(clean_smiles)\n",
    "df_train[TARGETS]  = df_train[TARGETS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df_test[\"SMILES\"]  = df_test[\"SMILES\"].str.strip().apply(clean_smiles)\n",
    "\n",
    "print(f\"Train shape  : {df_train.shape}\")\n",
    "print(f\"Test  shape  : {df_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4  Tokeniser (ChemBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def encode_smiles(smiles_list):\n",
    "    return tokenizer(\n",
    "        smiles_list,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "# pre-encode full datasets – keeps dataloader lightweight\n",
    "train_enc = encode_smiles(df_train[\"SMILES\"].tolist())\n",
    "test_enc  = encode_smiles(df_test[\"SMILES\"].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5  Dataset & DataLoader with masked labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolymerDataset(Dataset):\n",
    "    def __init__(self, encodings, target_df=None):\n",
    "        self.encodings = encodings\n",
    "        if target_df is not None:\n",
    "            self.targets = torch.tensor(target_df[TARGETS].values, dtype=torch.float32)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings[\"input_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        if self.targets is not None:\n",
    "            item[\"labels\"] = self.targets[idx]\n",
    "        return item\n",
    "\n",
    "full_ds = PolymerDataset(train_enc, df_train)\n",
    "test_ds = PolymerDataset(test_enc)\n",
    "\n",
    "# simple random 90 / 10 split for validation\n",
    "val_fraction = 0.1\n",
    "val_size     = int(len(full_ds) * val_fraction)\n",
    "train_size   = len(full_ds) - val_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size],\n",
    "                                                 generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6  Model – ChemBERTa backbone + 5-head regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task weights w_i: [2.1000e-03 6.2390e-01 2.2200e+00 1.0641e+00 4.6600e-02]\n"
     ]
    }
   ],
   "source": [
    "class ChemBERTaRegressor(nn.Module):\n",
    "    def __init__(self, model_name: str, n_targets: int = 5, hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.reg  = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, n_targets),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
    "        return self.reg(x)\n",
    "\n",
    "model   = ChemBERTaRegressor(MODEL_NAME, n_targets=len(TARGETS)).to(device)\n",
    "weights = compute_task_weights(df_train).to(device)\n",
    "print(\"Task weights w_i:\", weights.cpu().numpy().round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7  Masked-wMAE loss + metric helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmae_loss(outputs, targets, w):\n",
    "    mask = ~torch.isnan(targets)\n",
    "    abs_diff = torch.abs(outputs - targets)\n",
    "    weighted = abs_diff * w\n",
    "    return weighted[mask].mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    tot, cnt = 0.0, 0\n",
    "    for batch in loader:\n",
    "        outs = model(batch[\"input_ids\"].to(device),\n",
    "                     batch[\"attention_mask\"].to(device))\n",
    "        loss = wmae_loss(outs, batch[\"labels\"].to(device), weights)\n",
    "        tot += loss.item() * len(batch[\"input_ids\"])\n",
    "        cnt += len(batch[\"input_ids\"])\n",
    "    return tot / cnt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8  Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/448 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train wMAE ≈ 0.1304 | val wMAE = 0.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train wMAE ≈ 0.0853 | val wMAE = 0.0774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train wMAE ≈ 0.0777 | val wMAE = 0.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train wMAE ≈ 0.0753 | val wMAE = 0.0734\n"
     ]
    }
   ],
   "source": [
    "optim   = AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "steps   = len(train_loader) * EPOCHS\n",
    "sched   = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_warmup_steps=int(steps * WARMUP_RATIO),\n",
    "    num_training_steps=steps,\n",
    ")\n",
    "\n",
    "best_val = math.inf\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "        optim.zero_grad()\n",
    "        outs = model(batch[\"input_ids\"].to(device),\n",
    "                     batch[\"attention_mask\"].to(device))\n",
    "        loss = wmae_loss(outs, batch[\"labels\"].to(device), weights)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        sched.step()\n",
    "        running += loss.item()\n",
    "    val_wmae = evaluate(val_loader)\n",
    "    print(f\"Epoch {epoch:02d} | train wMAE ≈ {running/len(train_loader):.4f} |\"\n",
    "          f\" val wMAE = {val_wmae:.4f}\")\n",
    "    if val_wmae < best_val:\n",
    "        best_val = val_wmae\n",
    "        torch.save(model.state_dict(), OUTPUT_DIR / \"best.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9  Inference on test set & submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer: 100%|██████████| 1/1 [00:00<00:00, 24.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1109053969</td>\n",
       "      <td>0.077876</td>\n",
       "      <td>0.377792</td>\n",
       "      <td>0.219466</td>\n",
       "      <td>1.216206</td>\n",
       "      <td>1.405315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422188626</td>\n",
       "      <td>-0.005921</td>\n",
       "      <td>0.382061</td>\n",
       "      <td>0.244338</td>\n",
       "      <td>1.182076</td>\n",
       "      <td>1.396989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032016830</td>\n",
       "      <td>0.056237</td>\n",
       "      <td>0.377090</td>\n",
       "      <td>0.291676</td>\n",
       "      <td>1.115605</td>\n",
       "      <td>1.303566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        Tg       FFV        Tc   Density        Rg\n",
       "0  1109053969  0.077876  0.377792  0.219466  1.216206  1.405315\n",
       "1  1422188626 -0.005921  0.382061  0.244338  1.182076  1.396989\n",
       "2  2032016830  0.056237  0.377090  0.291676  1.115605  1.303566"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(OUTPUT_DIR / \"best.pt\"))\n",
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "    for batch in tqdm(test_loader, desc=\"Infer\"):\n",
    "        out = model(batch[\"input_ids\"].to(device),\n",
    "                    batch[\"attention_mask\"].to(device))\n",
    "        preds.append(out.cpu().numpy())\n",
    "preds = np.vstack(preds)\n",
    "\n",
    "sub = pd.DataFrame(preds, columns=TARGETS)\n",
    "sub.insert(0, \"id\", df_test[\"id\"])\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
